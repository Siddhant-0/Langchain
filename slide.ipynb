{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO1ujORZQuiAQxy8mSx8cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddhant-0/Langchain/blob/main/slide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52SGzU0VX2nx",
        "outputId": "45fe0717-9d7b-4504-ffc0-9335b28888a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-openai"
      ],
      "metadata": {
        "id": "e4MWVd_KdFWl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "id": "Ht_1qd8NlEBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe153c1-212e-472e-cd27-df21d46947d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "langsmith_api_key=userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "UAji4VkulIOh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\",api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "SIp-Vzpml4pg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(\"hi!\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsOGPspcHb_H",
        "outputId": "73bee915-a751-4ae2-ec18-3548dedfa31a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BgubT5AvbluYPgX1ANp0Jvyv37BEq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b712e949-5a3f-4238-9eaf-e32926454742-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(messages)\n",
        "print(response.content)     #printing the result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD2MtHeCHx7f",
        "outputId": "73212e91-7b47-42d8-e12a-23b4ad4a55cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same example as above (TRIAL)"
      ],
      "metadata": {
        "id": "o8ZwksLUHtoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# messages = [\n",
        "#     SystemMessage(\"Give the summary of the following message in about 20 words \"),\n",
        "#     HumanMessage(\"Nepal,[a] officially the Federal Democratic Republic of Nepal,[b] is a landlocked country in South Asia. It is mainly situated in the Himalayas, but also includes parts of the Indo-Gangetic Plain. It borders the Tibet Autonomous Region of China to the north, and India to the south, east, and west, while it is narrowly separated from Bangladesh by the Siliguri Corridor, and from Bhutan by the Indian state of Sikkim. Nepal has a diverse geography, including fertile plains, subalpine forested hills, and eight of the world's ten tallest mountains, including Mount Everest, the highest point on Earth. Kathmandu is the nation's capital and its largest city. Nepal is a multi-ethnic, multi-lingual, multi-religious, and multi-cultural state, with Nepali as the official language.\"),\n",
        "# ]\n",
        "\n",
        "# model.invoke(messages)"
      ],
      "metadata": {
        "id": "j6e-SredVw7a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response= model.invoke(messages)  # response has the result (content) along with its kwargs\n",
        "# content_result = response.content\n",
        "# # print(response)\n",
        "# print(\"Summary :  \",content_result)"
      ],
      "metadata": {
        "id": "UhW2pQzuGju3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat models that respond to user requests\n"
      ],
      "metadata": {
        "id": "vYyq4DGkINmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model inputs via strings or openAI format can also be done\n",
        "\n",
        "# model.invoke(\"Hello\")\n",
        "\n",
        "#! Has three roles : ->>>>>>>>>>>>>> User, System , Assistant\n",
        "\n",
        "model.invoke([{\"role\": \"user\", \"content\": \"Hi\"}])\n",
        "\n",
        "\n",
        "\n",
        "# model.invoke([HumanMessage(\"Hello\")])"
      ],
      "metadata": {
        "id": "BkCEMVPNXBtj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a32e98-70ff-4111-ad44-861cd9d34618"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bgubj3m4O2duBpVn7YxkeV8zOmQrY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a71af260-0ffe-468b-86ca-754608026282-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHAT MODELS AND PROMPT TEMPLATES TO DEVELPOP A APPLICATION"
      ],
      "metadata": {
        "id": "S35mBexLkd1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROMPT TEMPLATES --->\n",
        "Langchain concept that |take in raw user input and return data (a prompt) that is ready to pass into a language model."
      ],
      "metadata": {
        "id": "byFD0m1grP-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SYSTEM TEMPLATES\n",
        "\n",
        "* Messages were directly passed to the language model in above example.\n",
        "* The application logic takes the raw user input and transforms into a message that is raady to be passed to a language model  \n",
        "\n"
      ],
      "metadata": {
        "id": "zwUyJ1vdrjgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#takes two user variable -----------> text and language\n",
        "\n",
        "\n",
        "#class langchain_core.prompts.chat.ChatPromptTemplate   --->>source to ChatPromptTemplate\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "W7nGqiNlrUQl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html"
      ],
      "metadata": {
        "id": "iqL8WIjbeqeE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBpQcW_MeoJc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOLSJO831sbG",
        "outputId": "a229351b-558a-4bf9-867a-4ddc387684dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCzELhUm18YO",
        "outputId": "7a47a9fd-bc68-4b9f-9aed-78b3fbdb1431"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "# print(response)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUeIPLNp2Y89",
        "outputId": "024d0235-04dd-460d-bd0a-190029f67011"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ChatPromptTemplates\n"
      ],
      "metadata": {
        "id": "rZMoZOyAEyvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "#defining prompt template\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "#Give the template a topic\n",
        "prompt_value = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "response=model.invoke(prompt)\n",
        "# print(response)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSBjzuE0Eqpg",
        "outputId": "30a4d58d-c6e6-4cd3-aaa4-aedf4fcdf0a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatPrompt Templates\n"
      ],
      "metadata": {
        "id": "0zgM032fjeKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "    (\"human\",\"Tell me something about yourself\")  #helps it respond\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"name\": \"Bob\",\n",
        "        \"user_input\": \"What is your name?\"\n",
        "    }\n",
        ")\n",
        "\n",
        "#If not passed to a model the template invoked will return a list of input template response\n"
      ],
      "metadata": {
        "id": "gd0FzYTVjjeI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Response\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "response = model.invoke(prompt_value)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZeb6ZckA3KG",
        "outputId": "18f150e0-d14c-4c85-a2f7-0c06a716dffc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! My name is Bob, and I'm here to assist you with a wide range of topics, from answering questions to providing information and support. I have access to knowledge up until October 2023, so feel free to ask me anything! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RunnableParallel and Runnable concepts"
      ],
      "metadata": {
        "id": "q7P3emd0kyot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html\n",
        "\n",
        "https://python.langchain.com/docs/concepts/runnables/"
      ],
      "metadata": {
        "id": "9OlckcAdkxG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Standard Core Method Simple example\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def add_one(x: int) -> int:\n",
        "    return x + 1\n",
        "\n",
        "def mul_two(x):\n",
        "  return x * 2\n",
        "\n",
        "runnable_1 = RunnableLambda(add_one)\n",
        "runnable_2 = RunnableLambda(mul_two)\n",
        "sequence = runnable_1 | runnable_2\n",
        "\n",
        "\n",
        "\"\"\"for single\n",
        "sequence.invoke(1)\n",
        "await sequence.ainvoke(1)\"\"\"\n",
        "\n",
        "#concurrent run\n",
        "sequence.batch([1, 2, 3])\n",
        "await sequence.abatch([1, 2, 3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi4YJB9TmBVv",
        "outputId": "7ffa0ae4-9057-4407-966f-6d8b0aafb541"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 6, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple Runnable\n",
        "\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\"\"\"A RunnableSequence constructed using the `|` operator\"\"\"\n",
        "#Using lambda function\n",
        "\n",
        "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
        "sequence.invoke(1) # 4\n",
        "sequence.batch([1, 2, 3]) # [4, 6, 8]\n",
        "\n",
        "\n",
        "\"\"\"A sequence that contains a RunnableParallel constructed using a dict literal\"\"\"\n",
        "sequence = RunnableLambda(lambda x: x + 1) | {\n",
        "    'mul_2': RunnableLambda(lambda x: x * 2),\n",
        "    'mul_5': RunnableLambda(lambda x: x * 5)\n",
        "}\n",
        "sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEBYpVdWjoYu",
        "outputId": "b886d6fd-a98f-4c55-f77c-785483057a2a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mul_2': 4, 'mul_5': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NUJA9_LGo5RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# A RunnableSequence constructed using the `|` operator\n",
        "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
        "sequence.invoke(1) # 4\n",
        "sequence.batch([1, 2, 3]) # [4, 6, 8]\n",
        "\n",
        "\n",
        "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
        "sequence = RunnableLambda(lambda x: x + 1) | {\n",
        "    'mul_2': RunnableLambda(lambda x: x * 2),\n",
        "    'mul_5': RunnableLambda(lambda x: x * 5)\n",
        "}\n",
        "sequence.invoke(1) # Output : {'mul_2': 4, 'mul_5': 10}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjA0AzwNl5Ss",
        "outputId": "8054840f-f39a-4424-9578-b20dd24cd402"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mul_2': 4, 'mul_5': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Runnable Parrallel  --> singular input run concurrently to multiple functions\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# chain = RunnableLambda(lambda x : x + 1) | RunnableLambda(lambda x : x * 2 ) | RunnableLambda(lambda x : x * 3)\n",
        "\n",
        "# chain = chain = RunnableParallel(lambda x : x + 1,lambda x : x * 2 ,lambda x : x * 3)\n",
        "# chain.invoke(1)\n",
        "\n",
        "def add_one(x: int) -> int:\n",
        "    return x + 1\n",
        "\n",
        "def mul_two(x: int) -> int:\n",
        "    return x * 2\n",
        "\n",
        "def mul_three(x: int) -> int:\n",
        "    return x * 3\n",
        "\n",
        "runnable_1 = RunnableLambda(add_one)\n",
        "runnable_2 = RunnableLambda(mul_two)\n",
        "runnable_3 = RunnableLambda(mul_three)\n",
        "\n",
        "\"\"\"Incorrect as RunnableParallel expects a dictionary, not positional arguments.\n",
        "You must pass a dictionary where each key is a label for the function, and the\n",
        "value is the Runnable.\"\"\"\n",
        "# sequence = RunnableParallel(runnable_1, runnable_2, runnable_3)\n",
        "\n",
        "sequence =RunnableParallel(add_1=runnable_1,mul_2=runnable_2,mul_3=runnable_3)\n",
        "\n",
        "sequence.invoke(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbYmDNSjqhl5",
        "outputId": "4f6d6978-4f01-4e76-f1eb-1114c091b529"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'add_1': 2, 'mul_2': 2, 'mul_3': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OUTPUT PARSERS -\n",
        "  Output parser is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs."
      ],
      "metadata": {
        "id": "uhCKQpD9pwYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/concepts/output_parsers/"
      ],
      "metadata": {
        "id": "UhijP20ptphz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTE (JSON outpur parsers)\n",
        "JsonOutputParser implements the standard Runnable Interface"
      ],
      "metadata": {
        "id": "VCpfgdCxutis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \"\"\" Example by ----->Prabesh Dai\"\"\"\n",
        "\n",
        "\n",
        "# from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# from langchain.prompts import PromptTemplate\n",
        "\n",
        "# prompt = PromptTemplate.from_template(\"Extract relavant information about what user is asking for in the {query}.What is the topic of the question.What is the action the user is trying to ask.Give me answer in json format with keys topic and action\")\n",
        "\n",
        "# chain = prompt | model | SimpleJsonOutputParser()\n",
        "# outputs=10\n",
        "# topic=\"Prepare slides about software engineering\"\n",
        "# chain.invoke({\"query\":topic,\"outputs\":outputs})\n",
        "\n",
        "\n",
        "\"\"\"Improved version\"\"\"\n",
        "\n",
        "from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 1. Create a prompt template that takes 'query' as a variable\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Extract relevant information about what user is asking for in the query: {query}. \"\n",
        "    \"What is the topic of the question? What is the action the user is trying to ask? \"\n",
        "    \"Give the answer in JSON format with keys 'topic' and 'action'.\"\n",
        ")\n",
        "\n",
        "# 2. Initialize the model (DONE ABOVE)\n",
        "\n",
        "\n",
        "# 3. Create the chain: prompt -> model -> JSON parser\n",
        "chain = prompt | model | SimpleJsonOutputParser()\n",
        "\n",
        "# 4. Define the input\n",
        "query_text = \"Prepare slides about software engineering\"\n",
        "\n",
        "# 5. Invoke the chain with the query\n",
        "output = chain.invoke({\"query\": query_text})\n",
        "\n",
        "# 6. Print the result\n",
        "print(output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmjFq3d78lZW",
        "outputId": "d15ae5bf-6185-41e7-ab43-7be8c2c08cc9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'topic': 'software engineering', 'action': 'prepare slides'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Above example for combination of system template and prompt template\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")\n",
        "\n",
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "# print(response)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFNCqCR4njBx",
        "outputId": "0e608fde-a7e1-4694-c635-686a45cd97eb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PREPARATION OF SLIDES\n",
        "\n",
        "#CHAINING\n"
      ],
      "metadata": {
        "id": "s8aPzzUDrWff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# 1 Prompt template creation\n",
        "system_template = \"You are an intelligent assistant that extracts structured information.\"\n",
        "user_template = (\n",
        "    \"Given the user's input: '{query}', extract:\\n\"\n",
        "    \"- topic: what the input is about\\n\"\n",
        "    \"- action: what the user wants to do \\n\"\n",
        "    \" - information: what information do they want about the input \\n\"\n",
        "    \"Return the result in JSON with keys 'topic', 'action' and 'information'.\"\n",
        ")\n",
        "\n",
        "# 2. Combine into a chatPrompt\n",
        "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_template),(\"user\", user_template),])\n",
        "\n",
        "# 3. Model Initializaion\n",
        "\"\"\"Already set up above as   ----> model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\",api_key=openai_api_key)\"\"\"\n",
        "\n",
        "\n",
        "# 4. Set up the JSON parser\n",
        "parser = SimpleJsonOutputParser()\n",
        "\n",
        "# 5.\n",
        "chain = prompt_template | model | parser\n",
        "\n",
        "# 6. Define input\n",
        "query_text = \"Prepare slides about software engineering\"\n",
        "\n",
        "# 7. Run the chain\n",
        "output = chain.invoke({\"query\": query_text})\n",
        "\n",
        "# 8. Print the structured result\n",
        "print(output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g9OZ1vS6Fzf",
        "outputId": "74c49adf-1d91-40e2-c874-b351923c92fb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'topic': 'software engineering', 'action': 'prepare slides', 'information': 'slides about software engineering'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2fivPLJXmJc",
        "outputId": "88894be3-3274-4f49-e3fc-233c5789589e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper"
      ],
      "metadata": {
        "id": "sMqWthJ9sMjv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())"
      ],
      "metadata": {
        "id": "r_qLWF2AuQW2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_result = wikipedia.run(output[\"topic\"])\n",
        "print(wiki_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObVM3WT8ui13",
        "outputId": "6f39a116-05be-4c2e-d740-e04668bd3c7f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: Software engineering\n",
            "Summary: Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs.\n",
            "The terms programmer and coder overlap software engineer, but they imply only the construction aspect of a typical software engineer workload.\n",
            "A software engineer applies a software development process, which involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself.\n",
            "\n",
            "\n",
            "\n",
            "Page: History of software engineering\n",
            "Summary: The history of software engineering begins around the 1960s. Writing software has evolved into a profession concerned with how best to maximize the quality of software and of how to create it. Quality can refer to how maintainable software is, to its stability, speed, usability, testability, readability, size, cost, security, and number of flaws or \"bugs\", as well as to less measurable qualities like elegance, conciseness, and customer satisfaction, among many other attributes. How best to create high quality software is a separate and controversial problem covering software design principles, so-called \"best practices\" for writing code, as well as broader management issues such as optimal team size, process, how best to deliver software on time and as quickly as possible, work-place \"culture\", hiring practices, and so forth. All this falls under the broad rubric of software engineering.\n",
            "\n",
            "\n",
            "\n",
            "Page: Software development process\n",
            "Summary: In software engineering, a software development process or software development life cycle (SDLC) is a process of planning and managing software development. It typically involves dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design and/or product management. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.\n",
            "Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.\n",
            "A life-cycle \"model\" is sometimes considered a more general term for a category of methodologies and a software development \"process\" is a particular instance as adopted by a specific organization. For example, many specific software development processes fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TS7OQQgNv_uX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}